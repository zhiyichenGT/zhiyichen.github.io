---
layout: page
title: Methods
---

<h1 id="logistics">Methods<br>
</h1>
<p>We plan to divide the project into three stages: Exploratory Data Analysis (EDA), Supervised Learning, and Unsupervised Learning.
</p>
<h2 id="logistics">Dataset<br>
</h2>
<p>For this project, we will be using two datasets:
<br>
Covid Vaccine Tweets up until February 28, 2020
<br>
Stanford Sentiment Treeback
</p>

<h2 id="logistics">Exploratory Data Analysis (EDA)<br>
</h2>
<p>In the EDA, we plan to explore the geographical distributions of the tweets as well as other information such as the number of friends and followers of the users. We expect this information to provide us with some insights about the user such as his or her location and scale of social network on the internet.
</p>

<h2 id="logistics">Supervised Learning<br>
</h2>
<p>For supervised learning, we plan to divide it into two parts: (1) dictionary-based sentiment analysis on the tweets using SentiWords and VADER (Valence Aware Dictionary for Sentiment Reasoning); (2) training a model using the Stanford Sentiment Treebank (SST) and testing on the Covid Vaccine Tweets (CVT) dataset. 
</p>

<h3 id="logistics">Dictionary-based Sentiment Analysis<br>
</h3>
<p>
For this approach, we plan to calculate the sentiment scores based on different sentiment dictionaries and aggregate the word-level sentiment scores to obtain a document-level sentiment level on each tweet. We will compare the results of using different sentiment dictionaries as well as the model trained using the SST dataset as described below.
</p>

<h3 id="logistics">Neural Network-based Sentiment Analysis<br>
</h3>
<p>
For this part of the project, we will use the SST as our training data and the CVT as our testing data. This can also be divided into two parts. The first part will be fine-tuning pre-trained LSTM-based and Transformer-based models such as BERT using the SST dataset. The second part is to build an LSTM model as a baseline to compare the performances of these two models as well as the vanilla dictionary-based models described above.
<br>
To evaluate the result, we can compare the prediction of our model with the Dictionary-based results. In addition, we can also use Dictionary-based results as the labels of CVT and use the F1 score and confusion matrix to evaluate our model.
</p>
<p>
Here are several neural network models that we may use: Bi-directional LSTM, Bidirectional Encoder Representations from Transformers (BERT), etc.
<br>
Bi-directional LSTM is a typical model of Recurrent Neural Networks (RNN), which is a variant of Long Short Term Memory networks (LSTM). LSTMs have the chain like structure of RNN, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, which deal with different functions of memorizing or forgetting. While in Bi-directional LSTM, both Forward layer and Backward layer connect to the output layer, which makes the network can predict based on both two layers.
</p>
<p>
BERT is a new and famous model in NLP, which achieves best performances in multiple NLP problems. There are three interesting aspects of BERT: the first one is Masked Language Model, which randomly masks 15% tokens and only predicts these words in the final layer, so a sentence will be trained for multiple times. The second one is transformer, which contains several encoders and decoders with self-attention and multihead attention. The last one is sentence-level representation, which means that BERT not only uses the embeddings in token-level, but also learns segment embeddings and position embeddings to get the patterns of the sentence.
</p>

<h2 id="logistics">Unsupervised Learning<br>
</h2>

<h3 id="logistics">Topic Modeling<br>
</h3>
<p>
Aside from building a supervised classification model, we will check the nature of the data using unsupervised learning, performing experiments using different topic modeling algorithms, such as LSA (Latent Semantic Analysis), PLSA (Probabilistic Latent Semantic Analysis), LDA (Latent Dirichlet Allocation), and lda2Vec (a deep learning approach), to uncover the inherent topic upon which the approval or skepticism of COVID vaccine is built. This task mainly evolves around the tweets.</p>
<p>Common evaluation metrics such as silhouette score will be used. We will also use PCA (the first two principal components) and tSNE to visualize our high dimensional clustering result.
</p>
