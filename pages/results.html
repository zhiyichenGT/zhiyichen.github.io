---
layout: page
title: Results
---
<h1 id="logistics">Results<br>
</h1>

<h2 id="logistics">Exploratory Data Analysis (EDA)<br>
</h2>

<p>
    For the CVT dataset, we removed rows with any cell value that is inconsistent with the column name. For example, there are entries whose value for the <i>user_verified</i> column is neither <i>True</i> nor <i>False</i> but a random string as shown below.
</p>

<img src="../static/img/raw_covidvaccine.png" alt="Raw CVT Dataset" width="100%">

<p>
    Furthermore, we fixed the inconsistency in the format of DateTime values in the <i>date</i> column and expanded the list of hashtags into multiple attributes.
</p>

<img src="../static/img/covidvaccine.png" alt="CVT Dataset" width="100%">

<p>
    As mentioned earlier, we use data collected up until 2/28 23:59:00. Among all these data, the majority of data is generated in January and February 2021, with over 50000 tweets for each of the two months. For the rest of the months, the tweet counts range from less than 5000 to around 15000.
</p>
<center>
<img src="../static/img/tweet_count_by_month.png" alt="Tweet Count By Month" width="60%">
</center>
<p>
    Hashtag-wise, aside from the hashtag <i>#CovidVaccine</i>, most tweets have 0 or 1 other hashtags, as shown below.
</p>
<center>
    <img src="../static/img/tweet_count_by_number_of_hashtag.png" alt="Tweet Count by Number of Hashtags" width="60%">
</center>


<h2 id="logistics">Data Preprocessing<br>
</h2>
<p>
    To prepare our tweets data for training, we have established a series of preprocessing procedures:
</p>
<ol>
    <li><b>Hyperlink Removal</b>: Remove any hyperlink/URL in the tweet.</li>
    <li><b>HTML Stripping</b>: Strip off any HTML tag and header.</li>
    <li><b>Hashtag Removal</b>: Identify hashtags and either (1) remove the entire hashtag or (2) keep it by removing only the <i>#</i>.</li>
    <li><b>Punctuation Removal</b>: Remove all periods, commas, question marks, exclamation marks, colons, semi-colons, hyphens, and equal signs.</li>
    <li><b>Lowercasing</b>: Convert all letters to lowercase.</li>
    <li><b>Extra Newline Removal</b>: Remove any newline escape character (e.g. <i>\n</i> and <i>\t</i>).</li>
    <li><b>Contraction Expansion</b>: Use a pre-defined mapping to expand the contractions.</li>
    <li><b>Mis-spelled Word Fix</b>: Fix words that contain at least 3 repeated consecutive letter (since no words would contain letters repeating more than 2 times consecutively).</li>
    <li><b>Emoticon Replacement</b>: Use a pre-defined mapping to replace emotions to their names.</li>
    <li><b>Emoji Replacement</b>: Replace emojis with their names using <a href="https://pypi.org/project/emoji/"emoji</a> library.</li>
    <li><b>Stopword Removal</b>: Remove stopwords defined by <a href="https://www.nltk.org/">NLTK</a> library.</li>
    <li><b>Special Character Pattern Removal</b>: Remove any complete pairs of parentheses, brackets, and braces.</li>
    <li><b>Special Character Removal</b>: Keep only alphabetical and numeric characters and spaces.</li>
</ol>

<p>
    Most of the models used for this project will take as input the tokenized preprocessed text and convert them into different representations. For example, for our Dictionary-based Sentiment Analysis for the Supervised Learning part of the project, we will take the text tokens and directly map them into word-level sentiment scores. For the Latent Dirichlet Allocation in the Unsupervised Learning part of the project, the tokenized preprocessed text will be converted as Bag-of-Words and fed to the model.
</p>

<h2 id="logistics">Supervised Learning<br>
</h2>

<h3 id="logistics">Dictionary-based Sentiment Analysis<br>
</h3>

<p>
    We calculated the sentiment scores using the VADER (Valence Aware Dictionary for Sentiment Reasoning) python library to analyze the sentiment of tweets. VADER, a lexicon and rule-based sentiment analysis, is specifically used for sentiment analysis in social media, so it is a good choice for us to analyze tweets. 
</p>

<p>
    First, we created a VADER analyzer using the python library. VADER analyzer can calculate the negative, neutral, positive sentiment and a combination of these scores. The range of the first three scores is from 0 to 1, and the range of the combination is from -1 to +1. The combination score indicates negative sentiment with a negative value, positive sentiment with a positive value and neutral sentiment with a around zero value. After preprocessing the CVT dataset, we got the text of tweets to analyze. The results are shown below:
</p>

<center>
<img src="../static/img/VADER.png" alt="VADER Result" width="60%">
</center>

<h3 id="logistics">Next Step: SentiWords<br>
</h3>

<p>
    Next step we will use SentiWords to analyze sentiment on the tweets. We will use different sentiment dictionaries to calculate sentiment scores and compare their results for future purpose.
</p>

<h3 id="logistics">Neural Network-based Sentiment Analysis<br>
</h3>

<h4 id="logistics">Sentiment Analysis on Stanford Sentiment Treeback<br>
</h4>

<p>
    In this project, Covid Vaccine Tweets (CVT) dataset is unlabeled, while Stanford Sentiment Treeback (SST) dataset is a mature benchmark for sentiment analysis with well labeled data. So for the first part of the supervised learning, we implement three deep learning baselines: 1. Bi-directional LSTM without pretrained word embedding (denoted as Vanilla LSTM), 2. Bi-directional LSTM with Global Vectors for Word Representation (GLOVE) as pretrained word embedding (denoted as GLOVE LSTM), 3. fine-tuned Bidirectional Encoder Representations from Transformers (denoted as BERT) on SST dataset. The details about the three baselines are referred to Methods page. In addition, we transform the 0-4 labels to a ternary classification, which represents negative, neutral and positive sentiment respectively.
</p>

<p>
    We strictly follow the protocol of supervised learning: we select the hyper-paramater based on the performance on validation set firstly, and then, the final results of test set is reported when the model achieves best validation performance under the selected hyper-parameter. The performance is averaged over 3 runs. 
</p>

<h5 id="logistics">Hyper-parameter Selection<br>
</h5>

<p>
    For the reason that SST dataset is well prepared that the training set, validation set and test set are clearly divided, so we do not implement cross-validation here. In addition, from the original paper of BERT, we choose to select the learning rate of BERT from " 5e-5、3e-5、2e-5" and batch size from "16, 32". For two LSTM based methods, we choose learning rate from "0.1, 0.05, 0.01" and batch size from "16, 32".
</p>

<p>
    Considering to the limit space, we just report the final hyper-parameters for three models: BERT applies 2e-5 as learning rate and 32 as batch size, while two LSTM based model applies 0.05 as learning rate and 32 as batch size.
</p>

<h5 id="logistics">Accuracy and F1-score on SST<br>
</h5>

<p>
    We use average accuracy and F1-score as two metrics to judge the performance of different ML models. The results are reported below.
</p>

<center>
    <img src="../static/img/SST_baselines.png" alt="Performance of three baselines on SST" width="40%">
</center>

<p>
    Using Global Vectors for Word Representation (GLOVE) as pre-trained embedding obtains much better performances than Vanilla LSTM. The reason is that our dataset is much smaller than the dataset GLOVE used, so GLOVE makes more advantages from data. In addition, GLOVE is produced by global log-bilinear regression model based on co-occurrence matrix, which is not just learning through the backward propagation of neural network. As a result, GLOVE represents the words better, which contributes to better model performance. 
</p>

<p>
    However, BERT achieves the best performance comparing to two LSTM-based methods. The reason is that BERT uses multi-layer Transformers with attention, which makes the distance of all possible pairs becomes 1 in order to solve the problem of "Long-Term Dependencies" better than LSTMs (RNN based models). In addition, because of the fine-tune technique, not only we can use the architecture of BERT, but also use the learnt word embeddings (network parameters). So we save a lot of time of training a big network and get the best performance on SST as well. 
</p>

<h4 id="logistics">Sentiment Analysis on Stanford Sentiment Treeback<br>
</h4>

<p>
    From the first part of the supervised learning, we have observed that BERT performs best among three baselines on SST dataset, while using GLOVE pretrained embedding can improve the accuracy of vanilla model. So for the second part, we focus on Covid Vaccine Tweets (CVT) dataset and try to figure out how these three baselines perform on this bigger dataset.
</p>

<p>
    For the reason that CVT is unlabeled, we try to label the sentimental score based on two sentiment dictionary: Vader and SentimentWord. Here we only use Vader-based labels. And because CVT dataset is too big, we only go through it with one run.
</p>

<h5 id="logistics">Accuracy and F1-score on CVT<br>
</h5>

<p>
    We also use average accuracy and F1-score to judge the performance. The results are reported below.
</p>

<center>
    <img src="../static/img/CVT_baselines.png" alt="Performance of three baselines on CVT" width="40%">
</center>

<p>
    From the results, we can observe that the gap between BERT and GLOVE LSTM on CVT is much smaller than on SST. The reason is that the amount of data in CVT is almost 12 times of SST, which can take full advantage of the fitting ability of neural network. 
</p>

<p>
    In addition, GLOVE LSTM and BERT are better than vanilla LSTM, which indicates that good word embedding is very important in NLP.
</p>

<h5 id="logistics">Performance of the CVT-model on SST<br>
</h5>

<p>
    There are 98900 examples in the training set of CVT dataset, while the training set of SST only contains 8121 examples. So we want to see how a model which is trained on CVT dataset performs in SST dataset. Here the domains of label on two datasets are both {0, 1, 2} (Negative, Neutral, Positive). However, only BERT model can be directly tested on different dataset, because the domain of training data (X) is based on BertTokenizer, which ensures the same domain of two datasets. While for the other two LSTM-based models, the domain of X is tokenized only based on the training set, so domain is different. Here we only make observation on BERT model.
</p>

<center>
    <img src="../static/img/CVT_SST.png" alt="Performance of CVT-SST" width="40%">
</center>

<p>
    There is an obvious gap between the performances of BERT-CVT on test set of the two datasets. The reason is that the sentences of CVT are all about Covid19 Vaccine on tweeter, however, sentences of SST are reviews of films, which are very different on data domains. 
</p>

<p>
    For the future work, if we need to deal with the problem like this, we should use transfer learning and domain adaptation to mitigate the gap between the domain of X, which is a covariate shift.
</p>

<h2 id="logistics">Unsupervised Learning<br>
</h2>

<h3 id="logistics">Latent Dirichlet Allocation (LDA) using Tomotopy<br>
</h3>

<p>
    To uncover the inherent topics of our tweets, we apply Latent Dirichlet Allocation (LDA). This algorithm assumes that the documents observed, namely the tweets in our case, are generated based on two Dirichlet distributions. The first distribution models the probability of a given document being about a certain topic. The second one models the probability of each word given a certain topic. To optimize the LDA algorithm, Gibbs or Variational Bayes (VB) sampling is applied and tries to associate both each document and each word to as few (ideally one) topics as possible.
</p>

<p>
    There are two popular libraries that implement LDA: <a href="https://radimrehurek.com/gensim/models/ldamodel.html">gensim</a> and <a href="https://bab2min.github.io/tomotopy/v0.11.1/en/#tomotopy.LDAModel">tomotopy</a>. Tomotopy uses Collapsed Gibbs-Sampling(CGS) to infer the distribution of topics and the distribution of words. Generally CGS converges more slowly than Variational Bayes (VB) that Gensim’s LdaModel uses, but its iteration can be computed much faster. In addition, Tomotopy can take advantage of multicore CPUs with a SIMD instruction set, which can result in faster iterations. An example comparison shows that on a dataset consists of 1000 random documents from English Wikipedia with 1,506,966 words (about 10.1 MB). Tomotopy trains 200 iterations and Gensim trains 10 iterations. As a result, we decided to use Tomotopy’s LDA and fine-tune it.
</p>

<p>
    To fine-tune our model, we applied grid search on the number of topics, alpha (hyper-parameter for the document-topic Dirichlet distribution), and beta (hyper-parameter for the topics-word Dirichlet distribution). The number of topics ranges from 2 to 20, with a step size of 2. And both alpha and beta range from 0.01 to 0.91, with a step size of 0.1. To evaluate the results, we use Coherence Score, as shown below:
</p>
<center>
<img src="../static/img/sorted_lda_coherence.png" alt="LDA Coherence Result" width="50%">
</center>
<p>
    We evaluate the model using two types of coherence score: CV coherence score and the UMass coherence score. The former ranges from 0 to 1 and the latter ranges from -14 to 14. Both measure how well each topic (clusters) is by comparing the inter-topic relevance and intra-topic relevance. The higher the values the better. However, we found that when evaluating using the UMass coherence score, the highest scores are mostly achieved by having very few topics such as 2. As a result, we decided to use the CV coherence score as our primary metric to determine the best sets of hyper-parameters. The model performs the best when the number of topics is set to 20. And for each topics, we have provided the top-10 most relevant words:
</p>

<img src="../static/img/lda_topic_words.png" alt="LDA Topic Words" width="100%">

<p>
    We visualize the the relative importance of the topic words using word cloud:
</p>
<img src="../static/img/lda_wordcloud.png" alt="LDA word cloud" width="100%">

<p>
    As expected, the majority of Twitter users use the hashtag #CovidVaccine to talk about the latest progress in vaccine development or their experience in getting the shots. As we see in Topic #8 and Topic #18, they include the majority of words (meaning most of the content using these words are talking about vaccine development progress. Some keywords include "take", "get", "first", "dose", "covidvaccin", "pfizer", etc.
</p>

<p>
    However, we also found a few other interesting topics. Topic #2 revolves machine learning, deep learning, artificial intelligence. We’re guessing the topic is mainly about how to use big data to perform contact tracing and to slow the spread of the virus. Topic #7 is about the significant progress on vaccine development in Bangladesh and Topic #10 talks about Sheikh Hasina, president of Bangladesh, helping make the vaccine more accessible to Bangladeshis. Topic #19 discusses the postponement of the 2020 Summer Olympics at Tokyo, Japan. We sometimes discovered new topics when we re-ran LDA on the tweets data. For example, we have found topics talking about Apple’s stock price and MarketSmith, a stock research and investment tool.
</p>

<h3 id="logistics">Hierarchical Dirichlet Process (HDP) using Tomotopy<br>
</h3>
Tweet topics are intrinsically hierarchical, so we further apply Hierarchical Dirichlet Process (HDP).


<p>
    To fine-tune our model, we again applied grid search on the number of topics, alpha (hyper-parameter for the document-topic Dirichlet distribution), and beta (hyper-parameter for the topics-word Dirichlet distribution). The number of topics ranges from 2 to 20, with a step size of 2. And both alpha and beta range from 0.01 to 0.91, with a step size of 0.1. To evaluate the results, we use Coherence Score, as shown below:
</p>
<center>
    <img src="../static/img/sorted_hdp_coherence.png" alt="HDP Coherence Result" width="50%">
</center>

<p>
    The topic words are:
</p>
<center>
    <img src="../static/img/hdp_topic_words.png" alt="HDP topic words" width="100%">
</center>
<center>
    <img src="../static/img/hdp_wordcloud.png" alt="HDP word cloud" width="100%">
</center>